---
title: "The Art of Counting"
subtitle: "SSHOC Workshop 'Implementing Validation'"
author: "Andreas Blaette"
date: "September 30, 2019"
output:
  ioslides_presentation:
    css: css/stylesheet.css
    logo: img/polmine.png
    widescreen: yes
  slidy_presentation: default
editor_options:
  chunk_output_type: console
---

## The art of counting {.smaller}

- **Working with corpora is much more than counting.** But counting words and lexical units is the basic operation for any more complex analysis, and may yield substantial results.

- **To count is to measure!** Is my measurement valid? Are there sufficient safeguards that I do measure what I intend to measure?

- **Statements about salience means to take the difference between absolute and relative frequencies serious.**. Frequencies are the the normalisation of counts by dividing counts by corpus/subcorpus size.

- **There is a wide variety of scenarios for counting:** We will focus on time series analysis and dictionary-based analyses.

- Basic methods for counting in the polmineR  package are `count`, `dispersion` and `as.TermDocumentMatrix`. These methods are applicable for `corpus` and `subcorpus` objects. For the following example, we use the corpus of the verbatim records of the UN General Assembly.

```{r initialize, eval = TRUE, message = FALSE}
library(polmineR)
use("UNGA")
```


## Counting basics: The `count()`-method {.smaller}

- The most basic usage of the `count()`-method is to look up the number of occurrences of a search term (`query`) in a corpus.

```{r, collapse = TRUE}
count("UNGA", query = "refugees")
```

- The column `count` reports the absolute number of observations, the column `freq` the relative frequency. The frequency results from a simple division of the absolute count by the corpus size.

```{r, collapse = TRUE}
count("UNGA", query = "refugees")[["count"]] / size("UNGA")
```

- We can use a `character`-vector with several search terms.

```{r, collapse = TRUE}
count("UNGA", query = c("refugees", "asylum"))
```


## Using regular expressions and  CQP {.smaller}

- The `count()`-method will accept for the argument `query` the syntax of the Corpus Query Processor (CQP). One implication is that we can use regular expressions. The query needs to be put in single quotation marks, and the argument `cqp` is set to `TRUE`.

```{r, collapse = TRUE}
count("UNGA", query = "'refugee.*'", cqp = TRUE) # mit CQP-Syntax
```

- We can get a breakdown of the matches we have generated by setting the argument `breakdown` to `TRUE`.

```{r}
dt <- count("UNGA", query = "'refugee.*'", cqp = TRUE, breakdown = TRUE)
```

- The CQP syntax can also be used to match multi-word lexical units.

```{r}
dp <- count("UNGA", query = '"displaced" "persons"', cqp = TRUE)
```


## Matches for our regular expression {.smaller}

```{r, echo = FALSE}
DT::datatable(dt)
```


## Regular Expressions: Character classes


| Sign | Description |
|:-------:| --------------|
| .       | wildcard / matches any character |
| \\d | "digit" (0 to 9) |
| \\w | word character |
| \\s | whitespace |


## Regular Expressions: Quantifiers

| Sign | Description |
|:-------:| --------------|
|?| 	Zero or one occurrences of the preceding element.|
|+| One or more occurrences of the preceding element.  |
|*| Zero or more occurrences of the preceding element.|
|{n}| The preceding item is matched exactly n times. |
|{min,}| The preceding item is matched min or more times. |
|{min,max}| The preceding item is matched at least min times, but not more than max times. |


## Regular Expressions: Examples I {.smaller}

- This is sufficient to formulate queries that cover a broad set of cases.

```{r, collapse = TRUE}
count("UNGA", query = '"refuge.*"', cqp = TRUE, breakdown = TRUE) %>% head(n = 3)
```

- Alternative characters can be put in brackets ...

```{r, collapse = TRUE}
count("UNGA", query = '"[rR]efuge.*"', cqp = TRUE, breakdown = TRUE) %>% head(n = 3)
```


## Regular Expressions: Examples II {.smaller}

- Alternative formulations are wrapped in brackets and separated by the "|" sign. 

```{r, collapse = TRUE}
count("UNGA", query = '"(imm|em)igration.*"', breakdown = TRUE) %>% head()
```


## Querying the token stream {.smaller}

- Square brackets serve as a place holder for any token ...

```{r, collapse = TRUE}
count("UNGA", query = '"United" "Nations" "Commission" "for" []', cqp = T, breakdown = T)
```

- Curly brackets can be used as a quantifier ...

```{r, collapse = TRUE}
count("UNGA", query = '"[Rr]efugee.*" []{0,5} "burden"', cqp = TRUE, breakdown = TRUE) %>%
  head(n = 3) %>% subset(select = c("match", "count", "share"))
```


## Dispersion analysis {.smaller}

- Diachronic variation and synchronic change have a fundamental role for corpus analysis. 

- The `dispersion()`-method serves as a tool to efficiently count over one or two dimensions.


```{r get_simple_dispersion}
dt <- dispersion("UNGA", query = '"refugees"', s_attribute = "year")
head(dt) # wir betrachten nur den Anfang der Tabelle
```

- The `dispersion()`-method can process the CQP syntax just as the `count()`-method.


## Simple visualisation {.smaller}

```{r, echo = FALSE}
par(mfrow = c(1,1))
```

- Let us get frequencies from the outset.

```{r dispersion, message = FALSE}
dt <- dispersion("UNGA", query = '"refugee.*"', s_attribute = "year", freq = TRUE)
```

- We can visualise the result of our numerical analysis using a bar plot.

```{r, eval = FALSE}
barplot(
  height = dt[["freq"]] * 100000,
  names.arg = dt[["year"]],
  las = 2, ylab = "matches pro 100.000 Worte"
  )
```



## Refugees in the UN General Assembly {.flexbox .vcenter}

```{r, eval = TRUE, echo = FALSE}
barplot(
  height = dt[["freq"]] * 100000,
  names.arg = dt[["year"]],
  las = 2, ylab = "matches pro 100.000 Worte"
  )
```



## From numbers to words {.smaller}

* To obtain valid judgements about the meaning of counts (in a time-series analysis), it will be necessary to inspect the context for the matches obtained.

* The `kwic()`-method (for keyword-in-context) serves this purpose. The basic usage is as follows.

```{r}
kwic("UNGA", query = '"[Rr]efugee.*"', cqp = TRUE)
```

* A more realistic scenario will be this one ...

```{r, message = FALSE}
k <- corpus("UNGA") %>%
  subset(year == "2015") %>%
  kwic(
    query = '"[Rr]efugee.*"', cqp = TRUE,
    left = 10, right = 10,
    s_attributes = c("state_organization", "year")
    )
```


## Concordances {.smaller}

```{r, echo = FALSE}
options("polmineR.pagelength" = 7L)
```


```{r, render = knit_print, echo = FALSE, message = FALSE}
k
```


## Questions / concerns

* Which metadata (i.e. display of s-attributes) do you need for your statements?

* How much word context (left and right tokens) do you need to arrive at findings?

* How do you think you could achieve intersubjectivity for you findings?

* Which workflow would support your work?


## Annotating KWIC results {.smaller}

* First, we get the kwic lines ... 

```{r, message = FALSE}
refkwic <- corpus("UNGA") %>%
  subset(year == "1999") %>%
  kwic(query = '"refugee.*"', cqp = TRUE, left = 15, right = 15) %>%
  enrich(s_attribute = c("state_organization", "speaker"))
```

* Then we add an annotation layer ... 

```{r, eval = FALSE}
annotations(refkwic) <- list(name = "description", what = "")
```

* We then edit this result!

```{r, eval = FALSE}
edit(refkwic)
refkwic
```


## Conclusions





